# Homework 2 (ASR)

### Задание

Реализовать и обучить нейросеть для задачи распознавания речи (ASR).

**Вам нельзя пользоваться готовой реализацией из интернета или предобученной моделью.**

Мы предлагаем вам выполнить задание на основе шаблона [Project Template](https://github.com/Blinorot/pytorch_project_template). Вы можете выбрать любую из ветвей для своего проекта, однако мы рекомендуем вам использовать ветвь [`ASR`](https://github.com/Blinorot/pytorch_project_template/tree/example/asr).

---

### Требования

Мы **не принимаем** задание, если не выполнено какое-либо из следующих требований:

- Код должен храниться в публичном репозитории на `github` (или `gitlab`) и основываться на предоставленном шаблоне. До жесткого дедлайна используйте закрытый репозиторий и сделайте его публичным перед проверкой задания ассистентами.
- Все необходимые пакеты должны быть указаны в `requirements.txt`.
- Все необходимые ресурсы (например, **checkpoint модели** и **используемые LM**) должны быть доступны для загрузки с помощью скрипта. Укажите скрипт и пример его запуска в `README.md`.
- Вы должны реализовать все функции в `inference.py`, чтобы мы могли проверить ваше задание (см. раздел Тестирование).
- Ваши скрипты `inference.py` и `train.py` должны без проблем работать после выполнения всех команд в вашем руководстве по установке. Для проверки вы можете создать чистую среду (**conda env**) и запустить ваш проект, следуя вашим шагам установки.
- Вы должны предоставить журналы обучения (logs) вашей окончательной модели. Мы настоятельно рекомендуем вам использовать отчеты в `W&B` или `CometML`.

- Прикрепите краткий отчет в `W&B` или `github` (`gitlab`), в котором будет описано следующее:
  - Как воспроизвести вашу модель? (например: обучите 50 эпох с конфигурацией `train_1.yaml` и 50 эпох с `train_2.yaml`)
  - Прикрепите журналы обучения (logs) обученных моделей.
  - Как вы обучили свою окончательную модель?
  - Что вы пробовали?
  - Что сработало, а что нет?
  - Также прикрепите краткий обзор всех бонусных задач, которые вы реализовали.

---
### Оценивание

Ваша оценка будет складываться из оцениваемой метрики и качестве кода\отчета.

```
Оценка = ваш_скор - штрафы_за_реализацию - штрафы_за_отчет
```
---

### Штрафы за реализацияю

Мы требуем, чтобы были выолнены следующие требования:

- (До `-2.0 баллов`, если отсутствует) Логгирование. Ваши журналы `W&B` (`CometML`) должны включать:
  - Текстовые отчеты со случайными сэмплами: `target: {target_text}`, `predict: {prediction_text}`, `CER: {cer}`, `WER: {wer}`
  - Изображения ваших спектрограмм на трейне и на валидации     
  - Норму градиента
  - Learning rate
  - Loss
  - Аудиозаписи/спектрограммы (после аугментации)
  
(До `-1.0 балла`, если отсутствует) Реализуйте не менее 4 типов аугментаций, которые помогют при обучении ASR моделей.

### Quality score

| Score | Dataset                     | CER | WER | Description                                                                     |
| ----- | --------------------------- | --- | --- | ------------------------------------------------------------------------------- |
| 1.0   | --                          | --  | --  | At least you tried                                                              |
| 2.0   | LibriSpeech: test-clean     | 50  | --  | Well, it's something                                                            |
| 3.0   | LibriSpeech: test-clean     | 30  | --  | You can guess the target phrase if you try                                      |
| 4.0   | LibriSpeech: test-clean     | 20  | --  | It gets some words right                                                        |
| 5.0   | LibriSpeech: test-clean     | --  | 40  | More than half of the words are looking fine                                    |
| 6.0   | LibriSpeech: test-clean     | --  | 30  | It's quite readable                                                             |
| 7.0   | LibriSpeech: test-clean     | --  | 20  | Occasional mistakes                                                             |
| 8.0   | LibriSpeech: **test-other** | --  | 30  | Your network can handle somewhat noisy audio.                                   |
| 8.5   | LibriSpeech: **test-other** | --  | 25  | Your network can handle somewhat noisy audio but it is still just close enough. |
| 9.0   | LibriSpeech: **test-other** | --  | 20  | Somewhat suitable for practical applications.                                   |
| 9.5   | LibriSpeech: **test-other** | --  | 15  | You are close to human performance.                                             |
| 10.0  | LibriSpeech: **test-other** | --  | 10  | Technically better than a human. Well done!                                     |

> [!IMPORTANT]
> Ваши результаты будут проверены на корректность на необъявленном наборе данных. Поэтому не нужно подстраиваться под тестовые данные. 

### Бонусы
(+1.0 балл) Используйте внешнюю языковую модель (LM) для инференса. Примечание: реализация этой части даст очень значительный прирост качества (что значительно улучшит вашу метрику). 

(+1.0 балл) Используйте BPE токенизацию вместо букв. Вы можете использовать SentencePiece, HuggingFace или YouTokenToMe.

(до +3.0 балла) Обучите модель LAS (вместо CTC / с CTC). Не забудьте добавить в attention матрицы в логгирование.

> [!NOTE]
> Если вы использете бонус с LM, то вам разрешено брать предварительно обученную LM из интернета и использовать внешнюю библиотеку для `beam_search`. Также вы должны показать, что ваша LM работает.

> [!NOTE]
> Если вы используете `BPE`, то вам нужно провести два эксперимента с\без BPE и сравнить их (т. е. сделать `ablation study`).

> [!NOTE]
> Бонус LAS применяется к таблице в разделе [Quality Score](#quality-score), то есть ваша окончательная оценка качества будет $\max(Оценка CTC, Оценка LAS + 3)$.

### Бонусные баллы/штрафы
Мы можем вычесть или добавить баллы за очень плохую или очень хорошую структуру кода, нестандартные подходы, очень хороший отчет и т.д.

### Советы

#### Рекомендуемые архитектуры:

- [DeepSpeech2](http://proceedings.mlr.press/v48/amodei16.pdf)
- [QuartzNet](https://arxiv.org/abs/1910.10261). Примечание: сложно обучать без большого batch_size и хорошего GPU.
- [Jasper](https://arxiv.org/pdf/1904.03288.pdf). Примечание: сложно обучать без большого batch_size и хорошего GPU.
- [Conformer](https://arxiv.org/abs/2005.08100)

> [!IMPORTANT]
> Если у вас ограниченные ресурсы GPU, то используйте их эффективно. Мы предлагаем вам использовать бесплатные GPU в Google Colab (8 ч/день) и Kaggle (30 ч/неделя) для обучения. Все домашнее задание можно выполнить в Kaggle.

Обучение хорошей модели NN — сложная задача, которую крайне сложно отладить. Мы рекомендуем вам выполнить следующие шаги:

1. Переобучите свою модель на одном батче (One Batch Test).
2. Обучите свою модель на датасете  Librispeech (пока не достигнете как минимум 30 WER на test-clean Libirispeech).
3. Дообучите свою модель на смеси наборов датасетов Librispeech и Common Voice (или train-other Libirispeech), чтобы улучшить качество на test-other Libirispeech.

Рекомендуем начать выполнение задания локально (достаточно ресурсов ЦПУ):

0. Внимательно изучите каждый скрипт, это поможет вам понять структуру шаблона.
1. Заполните все `TODO` в шаблоне.
2. Протестируйте работу логгирования (`W&B` / `CometML`), создание датасета и даталоадера, инициализацию модели и трейнера.
3. Протестируйте входы и выходы модели: убедитесь, что они нужных размеров.
4. Запустите One-Batch-Test с простой моделью MLP. Убедитесь, что лосс близкок к нулю.

### Полезные ссылки

- [Аугментации](https://pytorch.org/audio/stable/transforms.html)

- [LM для Librispeech](https://www.openslr.org/11/)

- [Библиотека для CTC](https://github.com/kensho-technologies/pyctcdecode)